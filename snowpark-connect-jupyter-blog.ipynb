{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ‚ö° Zero Migration from Apache Spark to Snowflake Using Snowpark Connect\n",
        "\n",
        "## üöÄ **Same PySpark Code, Snowflake's Superior Engine**\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **What This Notebook Proves**\n",
        "\n",
        "This notebook demonstrates the **game-changing reality** of **zero-code migration** to run Apache Spark code on Snowflake's cloud-native engine:\n",
        "\n",
        "\n",
        "### ü™Ñ **The Zero Migration Magic**\n",
        "\n",
        "**Change just TWO variables** and instantly transform your Spark workloads to run on Snowflake:\n",
        "\n",
        "```python\n",
        "# üéõÔ∏è INSTANT TRANSFORMATION!\n",
        "USE_SNOWPARK_CONNECT = True   # üöÄ Unlock Snowflake's  engine\n",
        "CONDA_ENV_NAME = \"your-env-name\"  # Your existing environment works unchanged\n",
        "```\n",
        "### üèÜ **Why Snowpark Connect Transforms Your Business**\n",
        "\n",
        "#### üí∞ **Zero Migration Investment**\n",
        "- **100% Code Compatibility**: Every line of existing PySpark code runs unchanged\n",
        "- **Instant ROI**: No rewriting, no retraining, no project delays\n",
        "- **Risk-Free Transition**: Test workloads without any code modifications\n",
        "\n",
        "#### üöÄ **Snowflake's Superior Engine Advantages**\n",
        "- **‚ö° Elastic Auto-Scaling**: Automatic compute adjustment from small datasets to petabytes\n",
        "- **üåê Multi-Cloud Flexibility**: Run on AWS, Azure, or GCP without vendor lock-in\n",
        "- **üîí Enterprise Security**: Built-in governance, compliance, and data protection\n",
        "- **üí∏ Pay-per-Second Billing**: Only pay for actual compute consumption, not idle clusters\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üõ†Ô∏è **Zero Migration Setup: From Local Spark to Snowflake**\n",
        "\n",
        "\n",
        "### üìã **Step 1: Create Your Environment**\n",
        "\n",
        "#### **Install Snowpark Connect**\n",
        "```bash\n",
        "# üöÄ ENTERPRISE TRANSFORMATION READY!\n",
        "conda create -n snowpark-connect-demo python=3.12\n",
        "conda activate snowpark-connect-demo\n",
        "\n",
        "# Your PySpark dependencies\n",
        "conda install -c conda-forge pyspark matplotlib seaborn jupyter\n",
        "\n",
        "# üéØ Add Snowpark Connect for instant enterprise scaling\n",
        "pip install snowpark-connect\n",
        "```\n",
        "\n",
        "### üìã **Step 2: Unlock Snowflake's Enterprise Engine (Zero Code Changes Required)**\n",
        "\n",
        "**Experience the power of enterprise-scale analytics** with the same PySpark code:\n",
        "\n",
        "1. **üè¢ Snowflake Account** with Snowpark Connect enabled  \n",
        "   *‚Üí Instant access to elastic, multi-cloud compute*\n",
        "2. **‚ö° Snowflake CLI** for seamless authentication  \n",
        "   *‚Üí Enterprise security with zero complexity*\n",
        "\n",
        "#### **Install Snowflake CLI:**\n",
        "```bash\n",
        "pip install snowflake-cli-labs\n",
        "```\n",
        "\n",
        "#### **Configure Connection:**\n",
        "```bash\n",
        "# Create connection named 'spark-connect'\n",
        "snow connection add --connection-name spark-connect \\\n",
        "  --account your-account \\\n",
        "  --user your-username \\\n",
        "  --password \\\n",
        "  --database your-database \\\n",
        "  --schema your-schema \\\n",
        "  --warehouse your-warehouse\n",
        "```\n",
        "\n",
        "**üìñ Detailed Setup Guide:** \n",
        "- [Snowpark Connect Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-overview)\n",
        "- [Snowflake CLI Setup](https://docs.snowflake.com/en/developer-guide/snowflake-cli/connecting/specify-credentials)\n",
        "\n",
        "### üìã **Step 3: Configure This Notebook**\n",
        "\n",
        "Update the variables in the next cell with your settings:\n",
        "- Set `CONDA_ENV_NAME` to your environment name\n",
        "- Set `USE_SNOWPARK_CONNECT` to `True` if you want to use Snowflake\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Configuration\n",
        "\n",
        "Update the two variables in the next cell:\n",
        "- `USE_SNOWPARK_CONNECT`: True for Snowflake, False for local Spark\n",
        "- `CONDA_ENV_NAME`: your conda environment name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# üéõÔ∏è CONFIGURATION - UPDATE THESE FOR YOUR SETUP!\n",
        "# ===============================================\n",
        "\n",
        "# üîÑ ENGINE SELECTION - Choose your execution engine\n",
        "USE_SNOWPARK_CONNECT = True  # Set to True for Snowflake, False for Apache Spark\n",
        "\n",
        "# üêç ENVIRONMENT NAME - Update with your conda environment name\n",
        "CONDA_ENV_NAME = \"snowpark-connect-demo\"  # Replace with your actual environment name\n",
        "\n",
        "# ===============================================\n",
        "# Everything else is identical regardless of your settings! ü™Ñ\n",
        "# ===============================================\n",
        "\n",
        "# Auto-detect conda installation path (works on most systems)\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Try to detect conda path automatically\n",
        "if 'CONDA_PREFIX' in os.environ:\n",
        "    conda_base = os.environ['CONDA_PREFIX'].replace(f'/envs/{CONDA_ENV_NAME}', '')\n",
        "elif 'CONDA_DEFAULT_ENV' in os.environ:\n",
        "    conda_base = '/opt/anaconda3'  # Default location\n",
        "else:\n",
        "    # Common conda installation paths\n",
        "    potential_paths = [\n",
        "        '/opt/anaconda3',\n",
        "        '/opt/miniconda3', \n",
        "        os.path.expanduser('~/anaconda3'),\n",
        "        os.path.expanduser('~/miniconda3')\n",
        "    ]\n",
        "    conda_base = None\n",
        "    for path in potential_paths:\n",
        "        if os.path.exists(path):\n",
        "            conda_base = path\n",
        "            break\n",
        "    \n",
        "    if conda_base is None:\n",
        "        conda_base = '/opt/anaconda3'  # Fallback default\n",
        "\n",
        "# Construct environment path\n",
        "ENV_PATH = f\"{conda_base}/envs/{CONDA_ENV_NAME}\"\n",
        "\n",
        "# Display configuration\n",
        "print(\"üéõÔ∏è === NOTEBOOK CONFIGURATION ===\")\n",
        "print(f\"üîß Selected Engine: {'‚ùÑÔ∏è  Snowpark Connect (Snowflake)' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark (Local)'}\")\n",
        "print(f\"üêç Conda Environment: {CONDA_ENV_NAME}\")\n",
        "print(f\"üìÅ Environment Path: {ENV_PATH}\")\n",
        "print(f\"üåê Infrastructure: {'‚òÅÔ∏è  Cloud-native Snowflake' if USE_SNOWPARK_CONNECT else 'üíª Local computation'}\")\n",
        "\n",
        "# Verify environment exists\n",
        "if os.path.exists(ENV_PATH):\n",
        "    print(f\"‚úÖ Environment found: {ENV_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Environment not found at: {ENV_PATH}\")  \n",
        "    print(f\"üí° Please update CONDA_ENV_NAME or create the environment using setup instructions above\")\n",
        "\n",
        "print(f\"\\nüöÄ Ready to execute identical PySpark code!\")\n",
        "print(f\"ü™Ñ Same logic, {'different' if USE_SNOWPARK_CONNECT else 'local'} execution engine!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Environment configuration\n",
        "\n",
        "The following cell automatically configures Python paths based on your conda environment name. This ensures the notebook works with any properly named conda environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåç ENVIRONMENT CONFIGURATION - Uses your conda environment automatically!\n",
        "# This configuration works with ANY properly named conda environment\n",
        "\n",
        "import os\n",
        "\n",
        "# Clear any conflicting environment variables\n",
        "os.environ.pop('PYTHONPATH', None)\n",
        "\n",
        "# Set Python paths dynamically based on your environment name\n",
        "python_executable = f\"{ENV_PATH}/bin/python\"\n",
        "spark_home = f\"{ENV_PATH}/lib/python3.12/site-packages/pyspark\"\n",
        "\n",
        "# Configure PySpark environment variables\n",
        "os.environ['PYSPARK_PYTHON'] = python_executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = python_executable  \n",
        "os.environ['SPARK_HOME'] = spark_home\n",
        "\n",
        "print(\"üêç DYNAMIC Python environment configured:\")\n",
        "print(f\"   üéØ Environment: {CONDA_ENV_NAME}\")\n",
        "print(f\"   üêç Driver Python: {os.environ['PYSPARK_DRIVER_PYTHON']}\")\n",
        "print(f\"   üêç Worker Python: {os.environ['PYSPARK_PYTHON']}\")\n",
        "print(f\"   üè† Spark Home: {os.environ['SPARK_HOME']}\")\n",
        "print(f\"\\n‚úÖ Environment ready for: {'‚ùÑÔ∏è Snowpark Connect' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}\")\n",
        "\n",
        "# Verification\n",
        "if os.path.exists(python_executable):\n",
        "    print(f\"‚úÖ Python executable verified: {python_executable}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Python executable not found: {python_executable}\")\n",
        "    print(f\"üí° Please check your conda environment name and ensure it's created correctly\")\n",
        "\n",
        "print(f\"\\nüéâ Configuration complete! Ready for {'Snowflake' if USE_SNOWPARK_CONNECT else 'local Spark'} execution!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö **Library Imports**\n",
        "\n",
        "### üîÑ **Identical Imports for Both Engines**\n",
        "\n",
        "Notice how the imports are **exactly the same** regardless of which engine we're using.\n",
        "- Same DataFrame operations\n",
        "- Same SQL functions  \n",
        "- Same data types\n",
        "- Same visualization libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìö IDENTICAL imports - Same for both Apache Spark and Snowpark Connect!\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, when, round as spark_round, avg, sum as spark_sum, \n",
        "    count, max as spark_max, min as spark_min, \n",
        "    year, month, dayofweek, hour,\n",
        "    udf, lit\n",
        ")\n",
        "from pyspark.sql.types import StringType, FloatType, StructType, StructField, DoubleType, IntegerType, TimestampType\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"üìö libraries imported successfully!\")\n",
        "print(f\"üéØ Ready for: {'‚ùÑÔ∏è Snowpark Connect' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'} execution!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚ö° **The Transformation Moment: From Local to Enterprise**\n",
        "\n",
        "### üö® **THE ONLY CHANGE NEEDED FOR ZERO MIGRATION!**\n",
        "\n",
        "This is where the **enterprise magic happens**! One simple variable change transforms your Spark workload to run on Snowflake:\n",
        "\n",
        "| **Apache Spark** | `SparkSession.builder`\n",
        "\n",
        "| **‚ùÑÔ∏è Snowpark Connect** | `snowpark_connect.get_session()` \n",
        "\n",
        "\n",
        "### üèÜ **Why Snowpark Connect Changes Everything**\n",
        "- **üí∞ Zero Retraining Cost**: Your team's PySpark expertise instantly scales to enterprise\n",
        "- **‚ö° Instant Production**: No architectural changes, no code rewrites, no deployment complexity  \n",
        "- **üåê Enterprise Security**: Built-in governance without code modifications\n",
        "- **üìà Elastic Performance**: Auto-scale from development datasets to production petabytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö° ZERO MIGRATION TRANSFORMATION - Watch your Spark code run on Snowflake!\n",
        "\n",
        "if USE_SNOWPARK_CONNECT:\n",
        "    # ‚ùÑÔ∏è ENTERPRISE SNOWPARK CONNECT - THE GAME CHANGER!\n",
        "    print(\"‚ùÑÔ∏è Connecting to Snowflake's cloud-native engine...\")\n",
        "    print(\"üí° Same PySpark code, Snowflake infrastructure!\")\n",
        "    \n",
        "    try:\n",
        "        from snowflake import snowpark_connect\n",
        "        \n",
        "        # Enable enterprise Spark Connect mode\n",
        "        os.environ[\"SPARK_CONNECT_MODE_ENABLED\"] = \"1\"\n",
        "        \n",
        "        # Connect to Snowflake's superior engine\n",
        "        snowpark_connect.start_session()\n",
        "        spark = snowpark_connect.get_session()\n",
        "        \n",
        "        print(f\"\\nüéâ ENTERPRISE TRANSFORMATION COMPLETE!\")\n",
        "        print(f\"‚ö° Spark version: {spark.version}\")\n",
        "        print(f\"‚ùÑÔ∏è Execution engine: üèÜ SNOWFLAKE\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"‚ùå Missing Snowpark Connect package!\")\n",
        "        print(\"üí° Enable Snowpark Connect: pip install snowpark-connect\")\n",
        "        print(\"üìñ Setup guide: https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-overview\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Snowflake connection issue: {e}\")\n",
        "        print(\"üí° Verify your Snowflake CLI configuration:\")\n",
        "        print(\"   snow connection test -c spark-connect\")\n",
        "        raise\n",
        "        \n",
        "else:\n",
        "    # üü¶ LOCAL APACHE SPARK - DEVELOPMENT MODE\n",
        "    print(\"üü¶ === LOCAL DEVELOPMENT MODE ===\")\n",
        "    print(\"üíª Initializing local Spark session...\")\n",
        "    print(f\"‚ö†Ô∏è  LIMITED to local resources in environment: {CONDA_ENV_NAME}\")\n",
        "    \n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"NYC Taxi Analysis - Local Development\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Local Spark session created!\")\n",
        "    print(f\"‚ö° Spark version: {spark.version}\")\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "\n",
        "# üöÄ **ZERO MIGRATION PROOF**\n",
        "\n",
        "## üéØ **From Here Forward: Your Existing PySpark Code Unchanged!**\n",
        "\n",
        "Everything below this point is **100% identical PySpark code** that runs seamlessly on both local development and Snowflake. \n",
        "\n",
        "> **üèÜ Enterprise Migration Truth**: Every line of production PySpark code you've written will run unchanged on Snowflake's engine.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Data Processing**\n",
        "\n",
        "### üóΩ ** NYC Taxi Dataset**\n",
        "\n",
        "We'll process **50,000 realistic taxi trip records** to demonstrate enterprise-grade analytics capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèóÔ∏è IDENTICAL DATA GENERATION - Works on both engines!\n",
        "print(f\"üèóÔ∏è Generating NYC Taxi data using: {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}\")\n",
        "\n",
        "# IDENTICAL SCHEMA - Same structure for both engines\n",
        "schema = StructType([\n",
        "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
        "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
        "    StructField(\"passenger_count\", IntegerType(), True),\n",
        "    StructField(\"trip_distance\", DoubleType(), True),\n",
        "    StructField(\"fare_amount\", DoubleType(), True),\n",
        "    StructField(\"tip_amount\", DoubleType(), True),\n",
        "    StructField(\"payment_type\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# IDENTICAL DATA GENERATION ALGORITHM\n",
        "sample_data = []\n",
        "base_time = datetime(2023, 1, 15, 6, 0, 0)\n",
        "\n",
        "print(\"üìä Creating 50,000 sample taxi trip records...\")\n",
        "for i in range(50000):  # 50K records\n",
        "    # Realistic pickup times (6 AM to 11 PM peak distribution)\n",
        "    hour_offset = random.randint(0, 17)\n",
        "    minute_offset = random.randint(0, 59)\n",
        "    pickup_time = base_time.replace(hour=(base_time.hour + hour_offset) % 24, minute=minute_offset)\n",
        "    \n",
        "    # Realistic trip duration (3-60 minutes)\n",
        "    trip_duration_minutes = random.randint(3, 60)\n",
        "    dropoff_time = pickup_time + timedelta(minutes=trip_duration_minutes)\n",
        "    \n",
        "    # Realistic distance based on NYC traffic speed (8-25 mph)\n",
        "    base_speed = random.uniform(8, 25)\n",
        "    trip_distance = round((trip_duration_minutes / 60) * base_speed, 2)\n",
        "    \n",
        "    # NYC taxi fare structure: base + distance + time\n",
        "    base_fare = 2.50\n",
        "    distance_fare = trip_distance * random.uniform(2.40, 3.20)  # per mile\n",
        "    time_fare = trip_duration_minutes * random.uniform(0.30, 0.50)  # per minute\n",
        "    fare_amount = round(base_fare + distance_fare + time_fare, 2)\n",
        "    \n",
        "    # Realistic tipping behavior (0-25% of fare)\n",
        "    tip_percentage = random.uniform(0, 0.25)\n",
        "    tip_amount = round(fare_amount * tip_percentage, 2)\n",
        "    \n",
        "    sample_data.append((\n",
        "        pickup_time,\n",
        "        dropoff_time,\n",
        "        random.randint(1, 4),  # 1-4 passengers\n",
        "        trip_distance,\n",
        "        fare_amount,\n",
        "        tip_amount,\n",
        "        random.randint(1, 4)  # payment types: 1=Credit, 2=Cash, 3=No Charge, 4=Dispute\n",
        "    ))\n",
        "\n",
        "# IDENTICAL DATAFRAME CREATION\n",
        "taxi_df = spark.createDataFrame(sample_data, schema)\n",
        "\n",
        "print(f\"‚úÖ Generated realistic taxi dataset!\")\n",
        "print(f\"üìà Dataset Statistics (processed by {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}):\")\n",
        "print(f\"   üìä Total records: {taxi_df.count():,}\")\n",
        "print(f\"   üìã Columns: {len(taxi_df.columns)}\")\n",
        "\n",
        "# IDENTICAL SCHEMA DISPLAY\n",
        "print(f\"\\nüèóÔ∏è Schema (executed on {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}):\")\n",
        "taxi_df.printSchema()\n",
        "\n",
        "print(f\"\\nüéâ Data generation complete using {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'} engine!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üßπ **Data Cleaning and Feature Engineering**\n",
        "\n",
        "Now we'll implement the **complete data processing pipeline** using PySpark operations:\n",
        "\n",
        "#### üìã **Processing Steps:**\n",
        "1. **üßπ Data Quality Filters** - Remove invalid records and outliers\n",
        "2. **‚öôÔ∏è Feature Engineering** - Create derived columns for analysis  \n",
        "3. **üìä Data Validation** - Ensure realistic values and constraints\n",
        "\n",
        "### üéØ **Code Portability Demo**\n",
        "> These are **standard PySpark DataFrame operations** that work identically on both engines. Your existing data pipelines can switch engines with zero code changes!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üßπ Same filters and logic for both engines!\n",
        "print(f\"üßπ Applying data cleaning using: {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}\")\n",
        "\n",
        "# business rules\n",
        "clean_taxi_df = taxi_df.filter(\n",
        "    (col(\"trip_distance\") > 0) & \n",
        "    (col(\"trip_distance\") < 100) &  # Remove unrealistic long trips\n",
        "    (col(\"fare_amount\") > 0) & \n",
        "    (col(\"fare_amount\") < 500) &    # Remove unrealistic high fares\n",
        "    (col(\"passenger_count\") > 0) & \n",
        "    (col(\"passenger_count\") <= 6)   # Max 6 passengers in taxi\n",
        ")\n",
        "\n",
        "# derived columns\n",
        "enriched_taxi_df = clean_taxi_df.withColumn(\n",
        "    \"pickup_hour\", hour(col(\"tpep_pickup_datetime\"))\n",
        ").withColumn(\n",
        "    \"pickup_day_of_week\", dayofweek(col(\"tpep_pickup_datetime\"))\n",
        ").withColumn(\n",
        "    \"trip_duration_minutes\", \n",
        "    (col(\"tpep_dropoff_datetime\").cast(\"long\") - col(\"tpep_pickup_datetime\").cast(\"long\")) / 60\n",
        ").withColumn(\n",
        "    \"speed_mph\", \n",
        "    spark_round((col(\"trip_distance\") / (col(\"trip_duration_minutes\") / 60)), 2)\n",
        ")\n",
        "\n",
        "# realistic constraints\n",
        "final_taxi_df = enriched_taxi_df.filter(\n",
        "    (col(\"trip_duration_minutes\") > 1) & \n",
        "    (col(\"trip_duration_minutes\") < 300) &  # Max 5 hours\n",
        "    (col(\"speed_mph\") > 0) & \n",
        "    (col(\"speed_mph\") < 80)  # Reasonable speed limit for NYC\n",
        ")\n",
        "\n",
        "# RESULTS SUMMARY\n",
        "original_count = taxi_df.count()\n",
        "final_count = final_taxi_df.count()\n",
        "retention_rate = (final_count / original_count * 100)\n",
        "\n",
        "print(f\"üìä Data cleaning results (processed by {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}):\")\n",
        "print(f\"   üì• Original records: {original_count:,}\")\n",
        "print(f\"   üì§ After cleaning: {final_count:,}\")\n",
        "print(f\"   üéØ Data quality: {retention_rate:.1f}% retained\")\n",
        "print(f\"   üóëÔ∏è Filtered out: {original_count - final_count:,} invalid records\")\n",
        "print(f\"\\n‚úÖ Data cleaning completed using {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'} engine!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß **User-Defined Functions (UDFs)**\n",
        "\n",
        "\n",
        "Let's demonstrate how **UDF** runs identically on both engines\n",
        "\n",
        "#### üéØ **UDF Examples:**\n",
        "1. **`classify_trip_length()`** - Categorize trips by distance ranges\n",
        "2. **`calculate_tip_percentage()`** - Compute tip as percentage of fare\n",
        "\n",
        "### üèÜ **Enterprise Value**\n",
        "> Your existing UDF investments are **100% portable**! The same custom business logic, same function signatures, same results - regardless of the execution engine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß IDENTICAL UDF DEFINITIONS - Same business logic for both engines!\n",
        "\n",
        "def classify_trip_length(distance):\n",
        "    \"\"\"IDENTICAL FUNCTION - Classify trip length based on distance\"\"\"\n",
        "    if distance < 1:\n",
        "        return \"Short\"  # Quick neighborhood trips\n",
        "    elif distance < 5:\n",
        "        return \"Medium\"  # Typical city trips\n",
        "    elif distance < 15:\n",
        "        return \"Long\"    # Cross-borough trips\n",
        "    else:\n",
        "        return \"Very Long\"  # Airport/suburban trips\n",
        "\n",
        "def calculate_tip_percentage(tip_amount, fare_amount):\n",
        "    \"\"\"IDENTICAL FUNCTION - Calculate tip percentage with error handling\"\"\"\n",
        "    if fare_amount > 0:\n",
        "        return round((tip_amount / fare_amount) * 100, 2)\n",
        "    return 0.0\n",
        "\n",
        "# IDENTICAL UDF REGISTRATION - Same API for both engines\n",
        "classify_trip_udf = udf(classify_trip_length, StringType())\n",
        "tip_percentage_udf = udf(calculate_tip_percentage, FloatType())\n",
        "\n",
        "print(f\"üîß IDENTICAL User-Defined Functions registered on {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}:\")\n",
        "print(\"   üìè classify_trip_length() - Trip categorization logic\")\n",
        "print(\"   üí∞ calculate_tip_percentage() - Tipping behavior analysis\")\n",
        "print(\"   üéØ Both functions use identical business rules!\")\n",
        "\n",
        "# IDENTICAL UDF APPLICATION - Same DataFrame operations\n",
        "udf_taxi_df = final_taxi_df.withColumn(\n",
        "    \"trip_category\", classify_trip_udf(col(\"trip_distance\"))\n",
        ").withColumn(\n",
        "    \"tip_percentage\", tip_percentage_udf(col(\"tip_amount\"), col(\"fare_amount\"))\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ IDENTICAL UDFs applied successfully on {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}!\")\n",
        "print(f\"\\nüìã Sample with new columns (processed by {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}):\")\n",
        "\n",
        "# Show sample results with new UDF columns\n",
        "udf_taxi_df.select(\n",
        "    \"trip_distance\", \"trip_category\", \"fare_amount\", \"tip_amount\", \"tip_percentage\"\n",
        ").show(10)\n",
        "\n",
        "print(\"üéâ Notice: Same UDF logic, same results, different execution engine!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîç **Spark SQL Analytics**\n",
        "\n",
        "### üìä **Advanced Analytics - Engine Agnostic SQL**\n",
        "\n",
        "Now let's execute **complex SQL analytics** using the same queries on both engines. This demonstrates how your existing SQL investments are fully portable!\n",
        "\n",
        "#### üéØ **Analytical Goals:**\n",
        "1. **üìà Trip Statistics by Category** - Understand distance-based patterns\n",
        "2. **üïê Peak Hours Analysis** - Identify demand patterns by hour\n",
        "3. **üí≥ Payment Method Analysis** - Revenue and tipping insights\n",
        "\n",
        "### üí° **SQL Portability**\n",
        "> Every SQL query, every aggregation function, every analytical pattern runs **identically** on both engines. Your SQL expertise transfers seamlessly!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä IDENTICAL SQL SETUP - Same temporary view registration\n",
        "udf_taxi_df.createOrReplaceTempView(\"taxi_trips\")\n",
        "\n",
        "print(f\"üìä DataFrame registered as 'taxi_trips' view on {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}\")\n",
        "print(f\"üîç Ready for IDENTICAL SQL analytics!\")\n",
        "\n",
        "# üìà IDENTICAL SQL QUERY 1 - Trip statistics by category\n",
        "print(f\"\\nüöÄ Executing identical SQL on {'‚ùÑÔ∏è Snowflake cloud' if USE_SNOWPARK_CONNECT else 'üü¶ local Spark'}...\")\n",
        "\n",
        "trip_stats_sql = \"\"\"\n",
        "SELECT \n",
        "    trip_category,\n",
        "    COUNT(*) as trip_count,\n",
        "    ROUND(AVG(trip_distance), 2) as avg_distance,\n",
        "    ROUND(AVG(fare_amount), 2) as avg_fare,\n",
        "    ROUND(AVG(tip_percentage), 2) as avg_tip_pct,\n",
        "    ROUND(AVG(speed_mph), 2) as avg_speed,\n",
        "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pct_of_trips\n",
        "FROM taxi_trips \n",
        "GROUP BY trip_category\n",
        "ORDER BY trip_count DESC\n",
        "\"\"\"\n",
        "\n",
        "trip_stats_df = spark.sql(trip_stats_sql)\n",
        "print(f\"\\nüìà === Trip Statistics by Category ({'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'} Engine) ===\")\n",
        "trip_stats_df.show()\n",
        "\n",
        "# üïê IDENTICAL SQL QUERY 2 - Peak hours analysis  \n",
        "peak_hours_sql = \"\"\"\n",
        "SELECT \n",
        "    pickup_hour,\n",
        "    COUNT(*) as trip_count,\n",
        "    ROUND(AVG(fare_amount), 2) as avg_fare,\n",
        "    ROUND(AVG(trip_distance), 2) as avg_distance,\n",
        "    ROUND(AVG(speed_mph), 2) as avg_speed,\n",
        "    ROUND(AVG(tip_percentage), 2) as avg_tip_pct\n",
        "FROM taxi_trips \n",
        "GROUP BY pickup_hour\n",
        "ORDER BY pickup_hour\n",
        "\"\"\"\n",
        "\n",
        "peak_hours_df = spark.sql(peak_hours_sql)\n",
        "print(f\"\\nüïê === Hourly Trip Patterns ({'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'} Engine) ===\")\n",
        "peak_hours_df.show(24)\n",
        "\n",
        "# üí≥ IDENTICAL SQL QUERY 3 - Payment method analysis\n",
        "payment_analysis_sql = \"\"\"\n",
        "SELECT \n",
        "    CASE payment_type\n",
        "        WHEN 1 THEN 'Credit Card'\n",
        "        WHEN 2 THEN 'Cash'\n",
        "        WHEN 3 THEN 'No Charge'\n",
        "        WHEN 4 THEN 'Dispute'\n",
        "        ELSE 'Unknown'\n",
        "    END as payment_method,\n",
        "    payment_type,\n",
        "    COUNT(*) as trip_count,\n",
        "    ROUND(AVG(fare_amount), 2) as avg_fare,\n",
        "    ROUND(AVG(tip_amount), 2) as avg_tip,\n",
        "    ROUND(AVG(tip_percentage), 2) as avg_tip_pct,\n",
        "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage_of_trips\n",
        "FROM taxi_trips \n",
        "GROUP BY payment_type\n",
        "ORDER BY trip_count DESC\n",
        "\"\"\"\n",
        "\n",
        "payment_analysis_df = spark.sql(payment_analysis_sql)\n",
        "print(f\"\\nüí≥ === Payment Method Analysis ({'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'} Engine) ===\")\n",
        "payment_analysis_df.show()\n",
        "\n",
        "print(f\"\\n‚úÖ All SQL analytics completed successfully on {'‚ùÑÔ∏è Snowflake' if USE_SNOWPARK_CONNECT else 'üü¶ Apache Spark'}!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä IDENTICAL SUMMARY CALCULATIONS - Same aggregations for both engines!\n",
        "total_trips = udf_taxi_df.count()\n",
        "avg_fare = udf_taxi_df.select(avg(col(\"fare_amount\"))).collect()[0][0]\n",
        "avg_distance = udf_taxi_df.select(avg(col(\"trip_distance\"))).collect()[0][0]\n",
        "total_revenue = udf_taxi_df.select(spark_sum(col(\"fare_amount\"))).collect()[0][0]\n",
        "avg_tip_pct = udf_taxi_df.select(avg(col(\"tip_percentage\"))).collect()[0][0]\n",
        "avg_speed = udf_taxi_df.select(avg(col(\"speed_mph\"))).collect()[0][0]\n",
        "\n",
        "engine_name = \"‚ùÑÔ∏è SNOWFLAKE (Snowpark Connect)\" if USE_SNOWPARK_CONNECT else \"üü¶ APACHE SPARK (Local)\"\n",
        "infrastructure = \"‚òÅÔ∏è Cloud-native elastic compute\" if USE_SNOWPARK_CONNECT else \"üíª Local JVM process\"\n",
        "environment_info = f\"üìÅ Environment: {CONDA_ENV_NAME}\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"üöÄ === EXECUTION SUMMARY: {engine_name} ===\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìä Total Trips Processed: {total_trips:,}\")\n",
        "print(f\"üí∞ Average Fare: ${avg_fare:.2f}\")\n",
        "print(f\"üìè Average Distance: {avg_distance:.2f} miles\")\n",
        "print(f\"üíµ Total Revenue: ${total_revenue:,.2f}\")\n",
        "print(f\"üéØ Average Tip: {avg_tip_pct:.1f}%\")\n",
        "print(f\"üèÉ Average Speed: {avg_speed:.1f} mph\")\n",
        "print(f\"‚ö° Execution Engine: {engine_name}\")\n",
        "print(f\"üèóÔ∏è Infrastructure: {infrastructure}\")\n",
        "print(f\"{environment_info}\")\n",
        "print(f\"üîÑ Processing API: IDENTICAL PySpark DataFrame & SQL\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### üéØ **Complete Pipeline Execution Summary Using Snowpark Connect**\n",
        "\n",
        "\n",
        "### üèÜ **What We've Proven**\n",
        "- ‚úÖ **Same Code**: Zero changes required for running Spark code on Snowflake\n",
        "- ‚úÖ **Same Results**: Identical analytical outcomes\n",
        "- ‚úÖ **Same API**: PySpark DataFrame and SQL operations\n",
        "- ‚úÖ **Same Business Logic**: UDFs and custom functions work identically\n",
        "- ‚úÖ **Same Data Quality**: Processing pipelines transfer seamlessly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîó **Additional Resources**\n",
        "\n",
        "## üìñ **Setup Guides**\n",
        "- **Snowpark Connect**: [Official Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-overview)\n",
        "- **Snowflake CLI**: [Setup Guide](https://docs.snowflake.com/en/developer-guide/snowflake-cli/connecting/specify-credentials)\n",
        "\n",
        "# üôè **Thank You for Reading!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "snowpark-connect-s3-demo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
